{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee8b0d2",
   "metadata": {},
   "source": [
    "# what is Convolution Neural Network?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "935e1a56",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is commonly used for image recognition and processing tasks. It is designed to automatically learn and extract features from images, by applying a series of convolutional filters and pooling layers.\n",
    "\n",
    "To understand how CNNs work in real-time, let's consider the example of image classification. Suppose we want to train a CNN to recognize different types of animals in images. Here are the key steps involved:\n",
    "\n",
    "1.Data Collection:\n",
    "    Collect a large number of images of animals, such as dogs, cats, birds, etc.\n",
    "\n",
    "2.Preprocessing: \n",
    "    Resize and normalize the images to a standard size, convert them to grayscale or RGB, and split them into training, validation, and testing sets.\n",
    "\n",
    "3.Convolutional Layers:\n",
    "    The first layer of the CNN applies a set of filters to the input image, each of which detects a specific feature, such as edges, corners, textures, etc. The output of this layer is a set of feature maps, which represent the activations of the filters across the input image.\n",
    "\n",
    "4.Activation Function: \n",
    "    Apply a non-linear activation function, such as ReLU, to the output of the convolutional layer to introduce non-linearity and enhance the discriminative power of the features.\n",
    "\n",
    "5.Pooling Layers:\n",
    "    The next layer of the CNN applies a pooling operation, such as max-pooling or average pooling, to reduce the spatial size of the feature maps and extract the most salient features.\n",
    "\n",
    "6.Fully-Connected Layers: \n",
    "    The final layers of the CNN are fully connected layers, which take the flattened output of the previous layers and apply a set of weights to classify the input image into one of the categories.\n",
    "\n",
    "7.Training and Optimization:\n",
    "    Use backpropagation and gradient descent algorithms to adjust the weights of the CNN to minimize the classification error on the training set.\n",
    "\n",
    "8.Evaluation:\n",
    "    Test the performance of the CNN on the validation and testing sets, and fine-tune the model to improve its accuracy.\n",
    "\n",
    "Once the CNN is trained, it can be used in real-time to classify new images of animals, by applying the same set of convolutional filters and fully connected layers to extract and classify the features of the input image.\n",
    "\n",
    "For example, if we show the CNN a new image of a dog, it will apply its filters to detect the features of the dog, such as its fur, ears, nose, and eyes. The fully connected layers will then classify the image as a dog with a certain level of confidence. Similarly, if we show the CNN an image of a bird or a cat, it will classify it into the corresponding category.\n",
    "\n",
    "In this way, CNNs enable us to automate and accelerate the process of image recognition and processing, and can be used in a wide range of applications, such as self-driving cars, medical imaging, and security surveillance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c2c0e7c",
   "metadata": {},
   "source": [
    "Q: What is a Convolutional Neural Network (CNN)?\n",
    "A: A CNN is a type of deep neural network that is designed to process and classify images or other high-dimensional data.\n",
    "\n",
    "Q: What is the main difference between a CNN and a traditional neural network?\n",
    "A: The main difference is that a CNN applies a set of convolutional filters to the input data, which allows it to automatically learn and extract relevant features from images or other high-dimensional data.\n",
    "\n",
    "Q: What is a convolutional filter?\n",
    "A: A convolutional filter is a small matrix of numbers that is applied to an image or other high-dimensional data to extract specific features. The filter slides across the input data and performs a mathematical operation at each position, producing an output feature map.\n",
    "\n",
    "Q: What is pooling in a CNN?\n",
    "A: Pooling is a technique used in CNNs to downsample the output of the convolutional layer by taking the maximum or average value over a region of the feature map. This helps to reduce the size of the feature maps and extract the most important features.\n",
    "\n",
    "Q: What is a stride in a CNN?\n",
    "A: A stride is the number of pixels by which the convolutional filter slides over the input data. A larger stride results in a smaller output feature map, while a smaller stride results in a larger output feature map.\n",
    "\n",
    "Q: What is a fully connected layer in a CNN?\n",
    "A: A fully connected layer is a traditional neural network layer that takes the flattened output of the previous convolutional and pooling layers and applies a set of weights to classify the input data into one of several categories.\n",
    "\n",
    "Q: How are CNNs trained?\n",
    "A: CNNs are typically trained using backpropagation and gradient descent algorithms to minimize the difference between the predicted output and the actual output for a given input image or other high-dimensional data.\n",
    "\n",
    "Q: What are some applications of CNNs?\n",
    "A: CNNs are used in a wide range of applications, such as image recognition, object detection, face recognition, medical imaging, and natural language processing. They are particularly useful for tasks that involve large amounts of high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ec84c",
   "metadata": {},
   "source": [
    "# Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "649e8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3c6540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03454a71",
   "metadata": {},
   "source": [
    "# Part 1:Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358222e2",
   "metadata": {},
   "source": [
    "Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca70fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'C:/Users/thiru/Downloads/dataset (1)/dataset/training_set',\n",
    "        target_size = (64,64),\n",
    "        batch_size = 32,\n",
    "        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39e2a8",
   "metadata": {},
   "source": [
    "Preprocessing the testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bd7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_gen.flow_from_directory(\n",
    "        'C:/Users/thiru/Downloads/dataset (1)/dataset/test_set',\n",
    "        target_size = (64,64),\n",
    "        batch_size = 32,\n",
    "        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63ab00",
   "metadata": {},
   "source": [
    "# Part 2:Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda337a",
   "metadata": {},
   "source": [
    "Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4955f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b5314",
   "metadata": {},
   "source": [
    "# Step 1: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1080d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9a101",
   "metadata": {},
   "source": [
    "# step 2:Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217bcca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febda7cc",
   "metadata": {},
   "source": [
    "# Adding a Second Convolution Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb32ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3615e8",
   "metadata": {},
   "source": [
    "# Step 3:Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb88ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7ab4e",
   "metadata": {},
   "source": [
    "# Step 4:Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d86ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96e54a",
   "metadata": {},
   "source": [
    "# Step 5:Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7854b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8d355",
   "metadata": {},
   "source": [
    "# Part 3:Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e6d4f",
   "metadata": {},
   "source": [
    "Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc16a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5791885",
   "metadata": {},
   "source": [
    "# Training the CNN on the training set and evaluating it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f5ea28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.1781 - accuracy: 0.9296 - val_loss: 0.6699 - val_accuracy: 0.7870\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.1535 - accuracy: 0.9365 - val_loss: 0.6799 - val_accuracy: 0.7900\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.1489 - accuracy: 0.9434 - val_loss: 0.5921 - val_accuracy: 0.7970\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.1418 - accuracy: 0.9482 - val_loss: 0.6498 - val_accuracy: 0.7955\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.1348 - accuracy: 0.9503 - val_loss: 0.6987 - val_accuracy: 0.7840\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.1242 - accuracy: 0.9538 - val_loss: 0.7449 - val_accuracy: 0.7880\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.1225 - accuracy: 0.9536 - val_loss: 0.6646 - val_accuracy: 0.8015\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.1243 - accuracy: 0.9529 - val_loss: 0.6851 - val_accuracy: 0.7970\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.1152 - accuracy: 0.9579 - val_loss: 0.6972 - val_accuracy: 0.8030\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.1055 - accuracy: 0.9610 - val_loss: 0.7232 - val_accuracy: 0.7930\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.1013 - accuracy: 0.9616 - val_loss: 0.7733 - val_accuracy: 0.7840\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.0981 - accuracy: 0.9630 - val_loss: 0.7834 - val_accuracy: 0.7870\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.0964 - accuracy: 0.9654 - val_loss: 0.8522 - val_accuracy: 0.7855\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.0849 - accuracy: 0.9670 - val_loss: 0.8091 - val_accuracy: 0.7960\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.0845 - accuracy: 0.9685 - val_loss: 0.8211 - val_accuracy: 0.7870\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.0802 - accuracy: 0.9707 - val_loss: 0.8152 - val_accuracy: 0.7990\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.0771 - accuracy: 0.9710 - val_loss: 0.8724 - val_accuracy: 0.7855\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.0851 - accuracy: 0.9704 - val_loss: 0.8510 - val_accuracy: 0.7925\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.0802 - accuracy: 0.9718 - val_loss: 0.8688 - val_accuracy: 0.7840\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.0695 - accuracy: 0.9745 - val_loss: 0.8847 - val_accuracy: 0.7895\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.8478 - val_accuracy: 0.7905\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.0678 - accuracy: 0.9745 - val_loss: 0.9043 - val_accuracy: 0.7810\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.0672 - accuracy: 0.9751 - val_loss: 0.8985 - val_accuracy: 0.7940\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.0690 - accuracy: 0.9749 - val_loss: 0.9775 - val_accuracy: 0.7790\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.0517 - accuracy: 0.9816 - val_loss: 1.0065 - val_accuracy: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290b0427d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ab887",
   "metadata": {},
   "source": [
    "# Part 4: Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fece7fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28084/4100601845.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/thiru/Downloads/dataset (1)/dataset/test_set/dogs/dog.4144.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(\"C:/Users/thiru/Downloads/dataset (1)/dataset/test_set/dogs/dog.4144.jpg\", target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indeces\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d665dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img(\"C:/Users/thiru/Downloads/dataset (1)/dataset/test_set/dogs/dog.4144.jpg\", target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73e21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
